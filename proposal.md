# YOLO for ASL (proposal)
  
The 1 min 22 sec demo video proposal describes the genesis and vision for this project.  

Please unmute the video below if you'd like to hear sound or read the transcript that's posted just below the video.

<video src="https://user-images.githubusercontent.com/38410965/111876937-51b71a80-8977-11eb-8ee5-8698926db2c7.mp4" autoplay controls loop muted style="max-width: 730px;">
</video>

![proposal](https://user-images.githubusercontent.com/38410965/111876937-51b71a80-8977-11eb-8ee5-8698926db2c7.mp4)

> Hello everybody.  Last year, I did a project on American Sign Language during Maren's AI course, and I really enjoyed it.  So, this year I'd like to extend that to have some video.    
These guys, here (let me pull up their paper), these two fellows, here, did basically the same analysis  I did last Fall.  However, they built a video model, which ... the only problem with the video model that they have is that it takes them about (I don't know) about a minute to spell out the word 'babe' whereas one-shot-only models that are available on Apple *[actually from the [WWDC](https://developer.apple.com/videos/play/wwdc2018/717/); not 'on Apple']* might be able to get the job done more quickly.  
So, here's just an example of a model I built for my iPhone.  This is my iPhone on the left running something called 'breakfast finder', and you can just see how fast it's able to identify things.   
So, what I'd like to do is: try and do American Sign Language video classification using Apple CoreML, comparing that with AWS DeepLens, and then potentially having it follow me around in a [DonkeyCar](https://www.donkeycar.com) and classify the sign language as well.  We will see how it goes.  Thanks for watching.
